

from opto import trace
from opto.trainer.loader import DataLoader
from opto.trainer.algorithms import BasicSearchAlgorithm
from opto.optimizers import OptoPrimeV2
from opto.trainer.guide import AutoGuide
from opto.utils.llm import DummyLLM

import re, os
import numpy as np
import copy

@trace.bundle(trainable=True)
def fun(x):
    """ Some docstring. """
    return len(x), x.count('\n')

def test_saving_load():
    x = 'hello\nworld\n'
    a, b = fun(x)
    print(a, b)

    print(fun.parameters()[0].data)

    fun.parameters()[0]._data =fun.parameters()[0]._data.replace('len(x)', '"Hello"')

    a, b = fun(x)
    print(a, b)
    fun.save('fun.pkl')

    fun.load('fun.pkl')



    a, b = fun(x)
    print(a, b)


def test_trainer_saving_loading():


    class Guide(AutoGuide):

        def get_feedback(self, query, response, reference=None, **kwargs):
            """
            Provide feedback based on the query and response.

            Args:
                query: The query to analyze.
                response: The response generated by the model.
                reference: Optional reference answer for comparison.
                **kwargs: Additional context or parameters.

            Returns:
                A tuple containing a score and feedback string.
            """
            score = response == reference
            feedback = "Exact match!" if score == 1.0 else "Not an exact match."
            return score, feedback

    @trace.model
    class Agent:

        def __init__(self):
            self.param = trace.node(1., trainable=True)
            self.state = 0

        def forward(self, x):
            return self.param + 1


    xs = [1, 2, 3, 4, 5]
    infos = [1, 2, 3, 4, 5]
    batch_size = 3
    sub_batch_size = 2
    num_threads = 2 # 2
    dataset = {'inputs': xs, 'infos': infos}
    loader = DataLoader(dataset, batch_size=batch_size)
    num_proposals = 10
    num_candidates = 5
    memory_size = 3
    suggested_value = 5


    def _llm_callable(messages, **kwargs):
        """
        A dummy LLM callable that simulates a response.
        """
        problem = messages[1]['content']

        # extract name from <variable name= name ... >
        name = re.findall(r"<variable name=\"\s*(.*?)\" type=.*>", problem)
        if name:
            name = name[0]
        else:
            name = "unknown"

        return f"""
        <reasoning> Dummy reasoning based on the input messages. </reasoning>
        <variable>
        <name> {name} </name>
        <value> {suggested_value} </value>
        </variable>
        """

     # Create a dummy LLM and an agent
    dummy_llm = DummyLLM(_llm_callable)
    agent = Agent()
    optimizer = OptoPrimeV2(
        agent.parameters(),
        llm=dummy_llm,
    )
    optimizer.objective = 'fake objective'
    algo = BasicSearchAlgorithm(
        agent,
        optimizer,
    )

    algo.train(
        guide=Guide(),
        train_dataset=dataset,
        batch_size=batch_size,
        num_threads=num_threads,
        num_candidates=num_candidates,
        num_proposals=num_proposals,
        verbose=False, #'output',
    )
    agent.param._data = 10 # to simulate a change in the agent's parameters

    algo.save('test_algo.pkl')


    # Load the algorithm and check if it works
    agent = Agent()
    optimizer = OptoPrimeV2(
        agent.parameters(),
        llm=dummy_llm,
    )
    algo2 = BasicSearchAlgorithm(
        agent,
        optimizer,
    )
    algo2.load('test_algo.pkl')

    assert algo2.agent.param.data == 10, "Loaded agent's parameter does not match the saved one."
    assert algo2.optimizer.objective == 'fake objective', "Loaded optimizer's objective does not match the saved one."

    os.remove('test_algo.pkl')
    os.remove('test_algo.pkl_agent.module')
    os.remove('test_algo.pkl_optimizer.optimizer')
    os.remove('test_algo.pkl_validate_guide.guide')