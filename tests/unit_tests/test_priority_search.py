from opto import trace
from opto.trainer.loader import DataLoader
from opto.trainer.sampler import Sampler
from opto.trainer.algorithms.priority_search.priority_search import PrioritySearch as _PrioritySearch
from opto.trainer.algorithms.priority_search.priority_search import ModuleCandidate
from opto.optimizers import OptoPrimeV2
from opto.trainer.guide import AutoGuide
from opto.utils.llm import DummyLLM

import re
import numpy as np


class Guide(AutoGuide):

    def get_feedback(self, query, response, reference=None, **kwargs):
        """
        Provide feedback based on the query and response.

        Args:
            query: The query to analyze.
            response: The response generated by the model.
            reference: Optional reference answer for comparison.
            **kwargs: Additional context or parameters.

        Returns:
            A tuple containing a score and feedback string.
        """
        score = response == reference
        feedback = "Exact match!" if score == 1.0 else "Not an exact match."
        return score, feedback

@trace.model
class Agent:

    def __init__(self):
        self.param = trace.node(1., trainable=True)
        self.state = 0

    def forward(self, x):
        return self.param + 1



xs = [1, 2, 3, 4, 5]
infos = [1, 2, 3, 4, 5]
batch_size = 3
sub_batch_size = 2
num_threads = 2 # 2
dataset = {'inputs': xs, 'infos': infos}
loader = DataLoader(dataset, batch_size=batch_size, randomize=False)
sampler = Sampler(loader=loader, guide=Guide(), sub_batch_size=sub_batch_size, num_threads=num_threads)

num_proposals = 10
num_candidates = 5
memory_size = 3
suggested_value = 5



class PrioritySearch(_PrioritySearch):
    # This class is for testing the PrioritySearch algorithm

    def propose(self, samples, verbose=False, n_proposals=1, **kwargs):
        print("Propose at iteration:", self.n_iters)
        # assert len(samples) == batch_size, f"Expected {batch_size} samples, got {len(samples)}"
        # assert len(samples) == len(agents) * np.ceil(batch_size / self.sub_batch_size), f"Expected {len(agents) * np.ceil(batch_size / self.sub_batch_size)} samples, got {len(samples)}"

        candidates = super().propose(samples, verbose=verbose, n_proposals=n_proposals, **kwargs)
        # In this example this will always be value 5
        assert isinstance(candidates, list), "Expected candidates to be a list"
        assert all(isinstance(c, ModuleCandidate) for c in candidates), "All candidates should be ModuleCandidate instances"
        assert len(candidates) == np.ceil(batch_size / sub_batch_size) * self.num_proposals, f"Expected {np.ceil(batch_size / sub_batch_size) * self.num_proposals} candidates, got {len(candidates)}"
        return candidates

    def validate(self, candidates, samples, verbose=False, **kwargs):
        print("Validate at iteration:", self.n_iters)
        assert len(candidates) == np.ceil(batch_size / sub_batch_size) * self.num_proposals, f"Expected {np.ceil(batch_size / sub_batch_size) * self.num_proposals} candidates, got {len(candidates)}"

        validate_results = super().validate(candidates, samples, verbose=verbose, **kwargs)
        assert isinstance(validate_results, dict), "Expected validate_results to be a dict"
        assert all(isinstance(v, ModuleCandidate) for v in validate_results.keys()), "All keys should be ModuleCandidate instances"
        keys = list(validate_results.keys())
        # should contain one from exploration and one from exploitation
        assert len(validate_results) == 2, "In this example, all proposals are the same, so we expect only two validate results."

        return validate_results

    def exploit(self, **kwargs):
        print("Exploit at iteration:", self.n_iters)

        candidate, info_dict = super().exploit(**kwargs)
        assert isinstance(candidate, ModuleCandidate), "Expected candidate to be an instance of ModuleCandidate"
        assert isinstance(info_dict, dict), "Expected info_dict to be a dictionary"
        return candidate, info_dict

    def explore(self, **kwargs):
        print("Explore at iteration:", self.n_iters)

        candidates, info_dict = super().explore(**kwargs)
        assert isinstance(candidates, list)
        assert isinstance(info_dict, dict)

        if self.n_iters == 0:
            assert len(candidates) == 1, f"Expected 1 candidate, got {len(candidates)}"
        else:
            num_candidates = min(self.num_candidates, 2)  # in this example, memory will contain at most 2 unique candidates
            assert len(candidates) == num_candidates, f"Expected {num_candidates} candidates at iter {self.n_iters}, got {len(candidates)}"
        assert all(isinstance(c, ModuleCandidate) for c in candidates), "All candidates should be ModuleCandidate instances"

        return candidates, info_dict



def _llm_callable(messages, **kwargs):
    """
    A dummy LLM callable that simulates a response.
    """
    problem = messages[1]['content']

    # extract name from <variable name= name ... >
    name = re.findall(r"<variable name=\"\s*(.*?)\" type=.*>", problem)
    if name:
        name = name[0]
    else:
        name = "unknown"

    return f"""
    <reasoning> Dummy reasoning based on the input messages. </reasoning>
    <variable>
    <name> {name} </name>
    <value> {suggested_value} </value>
    </variable>
    """

dummy_llm = DummyLLM(_llm_callable)
agent = Agent()
optimizer = OptoPrimeV2(
    agent.parameters(),
    llm=dummy_llm,
)

algo = PrioritySearch(
    agent,
    optimizer,
)

algo.train(
    guide=Guide(),
    train_dataset=dataset,
    batch_size=batch_size,
    sub_batch_size=sub_batch_size,
    num_threads=num_threads,
    num_candidates=num_candidates,
    num_proposals=num_proposals,
    memory_size=memory_size,
    verbose=False,
)
