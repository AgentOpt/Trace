{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Code AND Prompts with Trace\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will demonstrate how to use the `trace` package to optimize prompts and code for natural language processing tasks using the BigBench-Hard benchmark. SotA approaches on this benchmark only optimize prompts, while relying on hand-written code to extract answers from LLM responses. By leveraging the LLM-based optimizers provided in `trace`, we aim to enhance the performance of a workflow calling LLMs and post-processing their responses in generating accurate and relevant answers.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we'll import the necessary packages and set up our environment. We will use a copy of the BigBench-Hard benchmark hosted on [HuggingFace](https://huggingface.co/datasets/maveriq/bigbenchhard). To use HuggingFace datasets, ensure that you have the `datasets` package installed:\n",
    "\n",
    "    git clone https://github.com/huggingface/datasets.git\n",
    "    cd datasets\n",
    "    pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adswamin\\AppData\\Local\\miniconda3\\envs\\newtrace\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import autogen\n",
    "from opto.trace.nodes import node, GRAPH, ParameterNode\n",
    "from opto.optimizers import FunctionOptimizerV2\n",
    "from datasets import load_dataset\n",
    "from textwrap import dedent\n",
    "from opto.trace.bundle import bundle\n",
    "from opto.trace.modules import model\n",
    "from typing import List\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Evaluation Function\n",
    "\n",
    "Next, we'll define the utility function for evaluating answers obtained by prompting an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metric(true, prediction):\n",
    "    matches = re.findall(r\"\\([A-Z]\\)\", true)\n",
    "    if matches:\n",
    "        pred = prediction\n",
    "        matches = re.findall(r\"\\([A-Z]\\)\", pred)\n",
    "        parsed_answer = matches[-1] if matches else \"\"\n",
    "        return parsed_answer == true\n",
    "    else:\n",
    "        return prediction == true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function\n",
    "\n",
    "We'll create a helper class called `LLMCallable` to interact with the LLM API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMCallable:\n",
    "    def __init__(self, config_list=None, max_tokens=1024, verbose=False):\n",
    "        if config_list is None:\n",
    "            config_list = autogen.config_list_from_json(\"OAI_CONFIG_LIST\")\n",
    "        self.llm = autogen.OpenAIWrapper(config_list=config_list)\n",
    "        self.max_tokens = max_tokens\n",
    "        self.verbose = verbose\n",
    "\n",
    "    @bundle(catch_execution_error=False)\n",
    "    def call_llm(self, user_prompt):\n",
    "        system_prompt = \"You are a helpful assistant.\\n\"\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "        response = self.llm.create(messages=messages, max_tokens=self.max_tokens)\n",
    "        response = response.choices[0].message.content\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"LLM response:\\n\", response)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Traced Class\n",
    "\n",
    "We will define a Predict class to generate predictions using LLM. Note that we use a module provided by `trace` called `Model` which can wrap a python class to enable tracing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model\n",
    "class Predict(LLMCallable):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.demos = []\n",
    "        self.prompt_template = dedent(\n",
    "            \"\"\"\n",
    "        Given the fields `question`, produce the fields `answer`.\n",
    "\n",
    "        ---\n",
    "\n",
    "        Follow the following format.\n",
    "\n",
    "        Question: \n",
    "        Answer: \n",
    "\n",
    "        ---\n",
    "        Question: {}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        )\n",
    "        self.prompt_template = ParameterNode(self.prompt_template, trainable=True,\n",
    "                                             description=\"[ParameterNode] This is the Prompt Template to the LLM. \" + \\\n",
    "                                                         \"Need to include information about what the format of answers LLM should output. \" + \\\n",
    "                                                         \"They can be (A)/(B), a number like 8, or a string, or Yes/No.\")\n",
    "\n",
    "    @bundle(trainable=True, catch_execution_error=True, allow_external_dependencies=True)\n",
    "    def extract_answer(self, prompt_template, question, response):\n",
    "        answer = response.split(\"Answer:\")[1].strip()\n",
    "        return answer\n",
    "\n",
    "    @bundle(trainable=True, catch_execution_error=True, allow_external_dependencies=True)\n",
    "    def create_prompt(self, prompt_template, question):\n",
    "        return prompt_template.format(question)\n",
    "\n",
    "    def forward(self, question):\n",
    "        user_prompt = self.create_prompt(self.prompt_template, question)\n",
    "        response = self.call_llm(user_prompt)\n",
    "        answer = self.extract_answer(self.prompt_template, question, response)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the optimizer\n",
    "\n",
    "Note that the `prompt_template` is a `ParameterNode` as well as the `extract_answer` is a trainable function. `trace` handles the optimization of heterogenous parameters seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_predict(dp, optimizer, examples):\n",
    "    for step, example in enumerate(examples):\n",
    "        GRAPH.clear()\n",
    "        try:\n",
    "            response = dp.forward(example['question'])\n",
    "            correctness = eval_metric(example['answer'], response)\n",
    "            feedback = \"The answer is correct! No need to change anything.\" if correctness else f\"The answer is wrong. We expect the output of your answer to be \\\"{example['answer']}\\\". Please modify the prompt and relevant parts of the program to help LLM produce the right answer.\"\n",
    "        except Exception as e:\n",
    "            response = str(e)\n",
    "            feedback = response\n",
    "            correctness = False\n",
    "            \n",
    "        print(\"Question:\", example[\"question\"])\n",
    "        print(\"Expected answer:\", example[\"answer\"])\n",
    "        print(\"Answer:\", response)\n",
    "\n",
    "        if correctness:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_feedback()\n",
    "        optimizer.backward(response, feedback)\n",
    "\n",
    "        print(f\"Output: {response}, Feedback: {feedback}, Variables:\")  # Logging\n",
    "        for p in optimizer.parameters:\n",
    "            print(p.name, p.data)\n",
    "        optimizer.step(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Finally, we use the optimizer to find better prompts using a small training set as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"sports_understanding\"\n",
    "train = load_dataset(\"maveriq/bigbenchhard\", task)[\"train\"]\n",
    "examples = [{\"question\": r[\"input\"], \"answer\": r[\"target\"]} for r in train]\n",
    "\n",
    "dp = Predict()\n",
    "optimizer = FunctionOptimizerV2(dp.parameters() + [dp.prompt_template],\n",
    "                                    config_list=autogen.config_list_from_json(\"OAI_CONFIG_LIST\"))\n",
    "\n",
    "print(\"Training on a few examples:\")\n",
    "learn_predict(dp, optimizer, examples[:5])\n",
    "    \n",
    "print(\"\\nTesting on new examples:\")\n",
    "for example in examples[5:10]:\n",
    "    response = dp.forward(example[\"question\"])\n",
    "    print(\"Question:\", example[\"question\"])\n",
    "    print(\"Expected answer:\", example[\"answer\"])\n",
    "    print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can run each cell in this notebook step by step to walk through the process of setting up and optimizing prompts for the trading game. Happy optimizing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
